{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook shows a simple Natural Language Processing (NLP) classification for identifying Tweets about natural disasters as real versus non-real or unrelated.  For details about the classification as a machine learning example, for the source datasets, or for other similar exampmle see:\n",
    "\n",
    "https://www.kaggle.com/c/nlp-getting-started\n",
    "\n",
    "Data exploration (EDA) is largely done prior to starting this notebook.  This focuses mostly on tokenization and data prep prior to model setup, generation and predictions.  The Keras Sequential ML model utilized here obviously is set with the most basic parameters, and no attempt has been made to optimize parameters or to compare the implementation of different alternative classification approaches.  However, validation accuracy for this first attempt (as shown) was over 92%.\n",
    "\n",
    "Note that other than source data, a Bing Maps API is required (and is sourced here from a local .env file). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import base64\n",
    "from dotenv import load_dotenv\n",
    "import geocoder\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "BING_MAP_KEY = base64.b64decode(os.getenv('BING_MAP_KEY')).decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission.csv', 'test.csv', 'test_data_with_locations.csv', 'train.csv', 'train_data_with_locations.csv']\n"
     ]
    }
   ],
   "source": [
    "#Show local tables to load into memory\n",
    "print(glob('*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                          1\n",
       "keyword                                                   NaN\n",
       "location                                                  NaN\n",
       "text        Our Deeds are the Reason of this #earthquake M...\n",
       "target                                                      1\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load and check training data\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "USA                104\n",
       "New York            71\n",
       "United States       50\n",
       "London              45\n",
       "Canada              29\n",
       "Nigeria             28\n",
       "UK                  27\n",
       "Los Angeles, CA     26\n",
       "India               24\n",
       "Mumbai              22\n",
       "Washington, DC      21\n",
       "Kenya               20\n",
       "Worldwide           19\n",
       "Chicago, IL         18\n",
       "Australia           18\n",
       "California          17\n",
       "California, USA     15\n",
       "New York, NY        15\n",
       "Everywhere          15\n",
       "Florida             14\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show most common locations for Tweets\n",
    "df_train.location.value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed line 6772...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Status code 400 from http://dev.virtualearth.net/REST/v1/Locations: ERROR - 400 Client Error: Bad Request for url: http://dev.virtualearth.net/REST/v1/Locations?q=++&o=json&inclnb=1&key=AqQxUDgDIW2P-Q9Yxna6W-XUMfo6GXMLdS3rHpzEsNAuMfFjTg_wWMI91p99Qnqc&maxResults=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed line 7613...\r"
     ]
    }
   ],
   "source": [
    "#Clean locations by geocoding and replacing with just country code\n",
    "#Bing Maps API is great geocoder tool, also Google works well\n",
    "count = 1\n",
    "def relocate(x):\n",
    "    global count\n",
    "    print(f'Processed line {count}...', end='\\r')\n",
    "    count += 1\n",
    "    if pd.isnull(x['location']):\n",
    "        return 'EMPTY'\n",
    "    g = geocoder.bing(x['location'], key=BING_MAP_KEY)\n",
    "    if not g.json is None:\n",
    "        try:\n",
    "            return g.json['country']\n",
    "        except:\n",
    "            return 'EMPTY'\n",
    "    else:\n",
    "        return 'EMPTY'\n",
    "\n",
    "df_train['country'] = df_train.apply(relocate, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean text that is very messy - return bases of words from Treebank tokens and WordNet Lemmatizer\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('[^a-zA-Z0-9 \\n\\.]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    lemmatizer=nltk.stem.WordNetLemmatizer()\n",
    "    text = \" \".join(lemmatizer.lemmatize(token) for token in tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EMPTY', 'United Kingdom', 'United States', 'South Africa',\n",
       "       'Hong Kong-China', 'Philippines', 'Canada', 'India', 'Barbados',\n",
       "       'Nigeria', 'Brazil', 'Australia', 'Germany', 'Kenya', 'Russia',\n",
       "       'Maldives', 'Switzerland', 'New Caledonia', 'Belgium', 'Indonesia',\n",
       "       'Belarus', 'Sri Lanka', 'France', 'Israel', 'Slovenia', 'Italy',\n",
       "       'Netherlands', 'Pakistan', 'Malaysia', 'Turkey', 'Spain',\n",
       "       'Argentina', 'Japan', 'Poland', 'Finland', 'Tuvalu', 'Cyprus',\n",
       "       'Mexico', 'Singapore', 'South Sudan', 'Burundi', 'Ireland',\n",
       "       'United Arab Emirates', 'West Bank', 'Cameroon', 'Mauritius',\n",
       "       'Norway', 'Latvia', 'Hungary', 'Peru', 'Belize', 'Austria',\n",
       "       'Trinidad and Tobago', 'Egypt', 'Ukraine', 'New Zealand', 'Greece',\n",
       "       'Sierra Leone', 'North Macedonia', 'Denmark', 'South Korea',\n",
       "       'Sweden', 'Iraq', 'Puerto Rico', 'Afghanistan', 'Saudi Arabia',\n",
       "       'Isle of Man', 'Golan Heights', 'Venezuela', 'Georgia', 'Colombia',\n",
       "       'Jamaica', 'Jersey', 'Hong Kong SAR', 'Portugal', 'Ecuador',\n",
       "       'Iceland', 'Bouvet Island', 'Andorra', 'Ethiopia', 'Bolivia',\n",
       "       \"CÃ´te d'Ivoire\", 'St Lucia', 'Uganda', 'Lithuania', 'Uruguay',\n",
       "       'Jordan', 'Libya', 'Congo (DRC)', 'Angola', 'Czechia', 'Romania',\n",
       "       'Chile', 'Honduras', 'Thailand', 'Somalia', 'Papua New Guinea',\n",
       "       'Dominican Republic', 'Ghana', 'Nepal', 'Morocco', 'Botswana',\n",
       "       'Lebanon', 'Mali', 'Kuwait', 'Bahrain', 'Zambia', 'Oman',\n",
       "       'Northern Mariana Islands', 'Vietnam', 'Moldova', 'Greenland',\n",
       "       'Sudan', 'China', 'Iran', 'Armenia', 'Mongolia', 'Zimbabwe',\n",
       "       'Cuba', 'Antarctica', 'Marshall Islands', 'Guatemala',\n",
       "       'Bangladesh', 'Panama', 'Turkmenistan', 'Bulgaria', 'Malawi',\n",
       "       'Kazakhstan', 'Serbia', 'Senegal', 'Antigua and Barbuda',\n",
       "       'Tunisia', 'Taiwan', 'Liberia', 'Syria', 'Namibia', 'Schweiz',\n",
       "       'Montenegro', 'Slovakia', 'Costa Rica', 'Suriname', 'Fiji'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.country.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONAL - write cleaned data to local file. Geocoding takes some time\n",
    "df_train.to_csv('train_data_with_locations.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['location'] = df_train['country']\n",
    "df_train.drop(labels='country', axis=1, inplace=True)\n",
    "df_train['text'] = df_train.apply(lambda x: clean_text(x['text']), axis=1)\n",
    "df_train.keyword.fillna('', inplace=True)\n",
    "df_train['keyword'] = df_train.apply(lambda x: clean_text(x['keyword']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our deed are the reason of this earthquake may allah forgive u all\n",
      "forest fire near la ronge sask canada\n",
      "all resident asked to shelter in place are being notified by officer no other evacuation or shelter in place order are expected\n",
      "people receive wildfire evacuation order in california\n",
      "just got sent this photo from ruby alaska a smoke from wildfire pours into a school\n",
      "rockyfire update california hwy closed in both direction due to lake county fire cafire wildfire\n",
      "flood disaster heavy rain cause flash flooding of street in manitou colorado spring area\n",
      "im on top of the hill and i can see a fire in the wood\n",
      "there an emergency evacuation happening now in the building across the street\n",
      "im afraid that the tornado is coming to our area\n"
     ]
    }
   ],
   "source": [
    "#Check cleaned text data\n",
    "for i in df_train.iloc[:10]['text'].values:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get word index for training data - ALL WORDS USED as returned by tokenizer\n",
    "vals = list(Counter([i for i in ' '.join(df_train.text.values.tolist()).split(' ')]).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As an option, take top n most used words \n",
    "counter = Counter([i for i in ' '.join(df_train.text.values.tolist()).split(' ')])\n",
    "top_words = [i for _, i in sorted(zip(counter.values(), counter.keys()), \n",
    "                                  key=lambda x: x[0], reverse=True)][:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vals = top_words\n",
    "indices = [i for i, _ in enumerate(vals)]\n",
    "word_index = {}\n",
    "values = {}\n",
    "for i in range(len(indices)):\n",
    "    word_index[indices[i]] = vals[i]\n",
    "    values[vals[i]] = indices[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=50000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'keyword', 'location', 'text', 'target'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n"
     ]
    }
   ],
   "source": [
    "#Get lines of text and convert to list of word keys\n",
    "texts = df_train.text.values\n",
    "sequences = []\n",
    "for line in texts:\n",
    "    test_line = [values[i.replace('#', '')] for i in line.split(' ') if i.replace('#', '') in vals]\n",
    "    sequences.append(test_line)\n",
    "print(len(sequences), sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert word keys for main text of tweet to categorical for training purposes\n",
    "first_part_train = vectorize_sequences(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 143)\n"
     ]
    }
   ],
   "source": [
    "#Perform tokenization for locations, and convert to binary/categorical arrays\n",
    "keyvals = list(Counter(df_train.location.values.tolist()).keys())\n",
    "indices = [i for i, _ in enumerate(keyvals)]\n",
    "loc_dict = {}\n",
    "for i in range(len(keyvals)):\n",
    "    loc_dict[keyvals[i]] = indices[i] + 1 #nan is 0\n",
    "locations = []\n",
    "for val in df_train.location.values:\n",
    "    if pd.isnull(val):\n",
    "        locations.append(0)\n",
    "    else:\n",
    "        locations.append(loc_dict[val])\n",
    "locations = to_categorical(np.array(locations), dtype='int32')\n",
    "print(locations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 179)\n"
     ]
    }
   ],
   "source": [
    "#Perform same categorization for keywords as for location values\n",
    "locvals = list(Counter(df_train.keyword.values.tolist()).keys())\n",
    "indices = [i for i, _ in enumerate(locvals)]\n",
    "key_dict = {}\n",
    "for i in range(len(locvals)):\n",
    "    key_dict[locvals[i]] = indices[i] + 1 #nan is 0\n",
    "keywords = []\n",
    "for val in df_train.keyword.values:\n",
    "    if pd.isnull(val):\n",
    "        keywords.append(0)\n",
    "    else:\n",
    "        keywords.append(key_dict[val])\n",
    "keywords = to_categorical(np.array(keywords), dtype='int32')\n",
    "print(keywords.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.hstack((first_part_train, locations, keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Develop model to train based on prepared input data\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='tanh', input_shape=(x_train.shape[1],)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizers.Nadam(),\n",
    "            loss=losses.binary_crossentropy, metrics=[metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7513 samples, validate on 100 samples\n",
      "Epoch 1/20\n",
      "7513/7513 [==============================] - 2s 309us/sample - loss: 0.5333 - acc: 0.7691 - val_loss: 0.2253 - val_acc: 0.9500\n",
      "Epoch 2/20\n",
      "7513/7513 [==============================] - 1s 167us/sample - loss: 0.3148 - acc: 0.8744 - val_loss: 0.1677 - val_acc: 0.9500\n",
      "Epoch 3/20\n",
      "7513/7513 [==============================] - 1s 168us/sample - loss: 0.1973 - acc: 0.9289 - val_loss: 0.1419 - val_acc: 0.9500\n",
      "Epoch 4/20\n",
      "7513/7513 [==============================] - 1s 168us/sample - loss: 0.1295 - acc: 0.9551 - val_loss: 0.1480 - val_acc: 0.9200\n",
      "Epoch 5/20\n",
      "7513/7513 [==============================] - 1s 168us/sample - loss: 0.0930 - acc: 0.9665 - val_loss: 0.1707 - val_acc: 0.9200\n",
      "Epoch 6/20\n",
      "7513/7513 [==============================] - 1s 168us/sample - loss: 0.0716 - acc: 0.9746 - val_loss: 0.1652 - val_acc: 0.9300\n",
      "Epoch 7/20\n",
      "7513/7513 [==============================] - 1s 168us/sample - loss: 0.0608 - acc: 0.9784 - val_loss: 0.1747 - val_acc: 0.9100\n",
      "Epoch 8/20\n",
      "7513/7513 [==============================] - 1s 169us/sample - loss: 0.0513 - acc: 0.9808 - val_loss: 0.1718 - val_acc: 0.9100\n",
      "Epoch 9/20\n",
      "7513/7513 [==============================] - 1s 167us/sample - loss: 0.0484 - acc: 0.9826 - val_loss: 0.2010 - val_acc: 0.9100\n",
      "Epoch 10/20\n",
      "7513/7513 [==============================] - 1s 169us/sample - loss: 0.0434 - acc: 0.9839 - val_loss: 0.1737 - val_acc: 0.9200\n",
      "Epoch 11/20\n",
      "7513/7513 [==============================] - 1s 169us/sample - loss: 0.0408 - acc: 0.9836 - val_loss: 0.2064 - val_acc: 0.9200\n",
      "Epoch 12/20\n",
      "7513/7513 [==============================] - 1s 169us/sample - loss: 0.0404 - acc: 0.9844 - val_loss: 0.2148 - val_acc: 0.9100\n",
      "Epoch 13/20\n",
      "7513/7513 [==============================] - 1s 168us/sample - loss: 0.0381 - acc: 0.9846 - val_loss: 0.2291 - val_acc: 0.9100\n",
      "Epoch 14/20\n",
      "7513/7513 [==============================] - 1s 168us/sample - loss: 0.0361 - acc: 0.9855 - val_loss: 0.2127 - val_acc: 0.9100\n",
      "Epoch 15/20\n",
      "7513/7513 [==============================] - 1s 167us/sample - loss: 0.0347 - acc: 0.9854 - val_loss: 0.2014 - val_acc: 0.9300\n",
      "Epoch 16/20\n",
      "7513/7513 [==============================] - 1s 164us/sample - loss: 0.0343 - acc: 0.9851 - val_loss: 0.1975 - val_acc: 0.9200\n",
      "Epoch 17/20\n",
      "7513/7513 [==============================] - 1s 164us/sample - loss: 0.0324 - acc: 0.9854 - val_loss: 0.2196 - val_acc: 0.9200\n",
      "Epoch 18/20\n",
      "7513/7513 [==============================] - 1s 164us/sample - loss: 0.0309 - acc: 0.9862 - val_loss: 0.2199 - val_acc: 0.9200\n",
      "Epoch 19/20\n",
      "7513/7513 [==============================] - 1s 164us/sample - loss: 0.0322 - acc: 0.9863 - val_loss: 0.2151 - val_acc: 0.9200\n",
      "Epoch 20/20\n",
      "7513/7513 [==============================] - 1s 162us/sample - loss: 0.0296 - acc: 0.9875 - val_loss: 0.2288 - val_acc: 0.9200\n"
     ]
    }
   ],
   "source": [
    "x_val = x_train[-100:]\n",
    "partial_x_train = x_train[:-100]\n",
    "y_val = y_train[-100:]\n",
    "partial_y_train = y_train[:-100]\n",
    "model.compile(optimizer='nadam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=50, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As an optional step, save trained model for reuse in the future\n",
    "model.save('model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3263, 143)\n",
      "(3263, 179)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Go through and apply same text tokens, as well as keyword and location tokens, from the training dataframe.\n",
    "Tokens won't be remade, to satisfy specific input shape of learning model.\n",
    "\"\"\"\n",
    "\n",
    "count = 1\n",
    "test_df['country'] = test_df.apply(relocate, axis=1)\n",
    "test_df['location'] = test_df['country']\n",
    "test_df.drop(labels='country', axis=1, inplace=True)\n",
    "test_df['text'] = test_df.apply(lambda x: clean_text(x['text']), axis=1)\n",
    "test_df.keyword.fillna('', inplace=True)\n",
    "test_df['keyword'] = test_df.apply(lambda x: clean_text(x['keyword']), axis=1)\n",
    "\n",
    "\n",
    "test_df['keyword'].fillna('', inplace=True)\n",
    "\n",
    "texts = test_df.text.values\n",
    "sequences = []\n",
    "for line in texts:\n",
    "    test_line = [values[i] for i in str(line).split(' ') if i in vals]\n",
    "    sequences.append(test_line)\n",
    "#Convert word keys for main text of tweet to categorical (this time for test purposes)\n",
    "first_part_test = vectorize_sequences(sequences)\n",
    "#Now to get location values (if matching ones exist in training dataset)\n",
    "locations = []\n",
    "for val in test_df.location.values:\n",
    "    if pd.isnull(val):\n",
    "        locations.append(0)\n",
    "    else:\n",
    "        try:\n",
    "            locations.append(loc_dict[val])\n",
    "        except KeyError:\n",
    "            locations.append(len(loc_dict))\n",
    "locations = to_categorical(np.array(locations), dtype='int32')\n",
    "print(locations.shape)\n",
    "#And finally, get keywords (again, only if matching keywords were present in training dataset)\n",
    "keywords = []\n",
    "for val in test_df.keyword.values:\n",
    "    if pd.isnull(val):\n",
    "        keywords.append(0)\n",
    "    else:\n",
    "        try:\n",
    "            keywords.append(key_dict[val])\n",
    "        except KeyError:\n",
    "            keywords.append(len(key_dict))  #value must be absent, but make sure tensor has same shape \n",
    "keywords = to_categorical(np.array(keywords), dtype='int32')\n",
    "print(keywords.shape)\n",
    "\n",
    "#As before, merge text tokens, location and keyword category values \n",
    "x_test = np.hstack((first_part_test, locations, keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3263/3263 [==============================] - 1s 156us/sample\n"
     ]
    }
   ],
   "source": [
    "#With the tensors carefully prepared, run predictions. Perform additional check to make sure shape matches.\n",
    "if x_test.shape[1] != x_train.shape[1]:\n",
    "    print('Error in processing - input matrix needs same number of columns as training data')\n",
    "else:\n",
    "    output_targets = model.predict(x_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       0\n",
       "2   3       0\n",
       "3   9       0\n",
       "4  11       0"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = pd.read_csv('sample_submission.csv')\n",
    "samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_aslist = [int(i) for i in output_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_table = pd.DataFrame({'id': test_df.id.values, 'target': output_aslist})\n",
    "output_table.to_csv('submissions.csv', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
